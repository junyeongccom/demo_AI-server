import pandas as pd
import numpy as np
import re
from pathlib import Path
from sklearn.model_selection import StratifiedKFold, GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder
from sklearn import __version__ as sklearn_version
from packaging import version
# XGBoost, LightGBM ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
try:
    import xgboost as xgb
    from xgboost import XGBClassifier
except ImportError:
    print("âš ï¸ XGBoost ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. 'pip install xgboost' ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    XGBClassifier = None

try:
    import lightgbm as lgb
    from lightgbm import LGBMClassifier
except ImportError:
    print("âš ï¸ LightGBM ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. 'pip install lightgbm' ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    LGBMClassifier = None

from app.domain.model.titanic_schema import TitanicSchema

"""
PassengerId  ê³ ê°ID,
Survived ìƒì¡´ì—¬ë¶€,
Pclass ìŠ¹ì„ ê¶Œ 1 = 1ë“±ì„, 2 = 2ë“±ì„, 3 = 3ë“±ì„,
Name,
Sex,
Age,
SibSp ë™ë°˜í•œ í˜•ì œ, ìë§¤, ë°°ìš°ì,
Parch ë™ë°˜í•œ ë¶€ëª¨, ìì‹,
Ticket í‹°ì¼“ë²ˆí˜¸,
Fare ìš”ê¸ˆ,
Cabin ê°ì‹¤ë²ˆí˜¸,
Embarked ìŠ¹ì„ í•œ í•­êµ¬ëª… C = ì‰ë¸Œë£¨, Q = í€¸ì¦ˆíƒ€ìš´, S = ì‚¬ìš°ìŠ¤í–„íŠ¼
"""

class TitanicService:
    def __init__(self, data_dir: str = './app/store/'):
        self.dataset = TitanicSchema(
            context=data_dir,
            fname='',
            train=None,
            test=None,
            id='',
            label=''
        )
        self.test_ids = None

    def new_model(self, fname: str) -> pd.DataFrame:
        path = Path(self.dataset.context) / fname
        return pd.read_csv(path)

    def preprocess(self, train_fname: str, test_fname: str, scaler_type: str = 'standard') -> TitanicSchema:
        """
        íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            train_fname: í•™ìŠµ ë°ì´í„° íŒŒì¼ëª…
            test_fname: í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ëª…
            scaler_type: ì •ê·œí™” ë°©ì‹ ('standard' ë˜ëŠ” 'minmax')
            
        Returns:
            ì „ì²˜ë¦¬ëœ TitanicSchema ê°ì²´
        """
        print("-------- ëª¨ë¸ ì „ì²˜ë¦¬ ì‹œì‘ --------")
        ds = self.dataset
        ds.train = self.new_model(train_fname)
        ds.test = self.new_model(test_fname)
        ds.id = ds.test['PassengerId']  # ì˜ˆì¸¡ ê²°ê³¼ ì œì¶œìš©ìœ¼ë¡œ ID ì €ì¥
        self.test_ids = ds.test['PassengerId']  # test_ids ì†ì„± ì„¤ì •
        ds.label = ds.train['Survived']
        ds.train = ds.train.drop('Survived', axis=1)
        
        # PassengerId ì œê±° (í•™ìŠµì— ë¶ˆí•„ìš”)
        ds = self.drop_feature(ds, 'PassengerId')
        
        # FamilySize ë° IsAlone íŠ¹ì„± ì¶”ê°€
        ds = self.add_family_features(ds)
        
        # Nameì—ì„œ Title ì¶”ì¶œ
        ds = self.extract_title_from_name(ds)
        title_mapping = self.remove_duplicate_title(ds)
        ds = self.title_nominal(ds, title_mapping)
        ds = self.drop_feature(ds, 'Name')
        
        # ì„±ë³„(Sex) ì²˜ë¦¬
        ds = self.gender_nominal(ds)
        ds = self.drop_feature(ds, 'Sex')
        
        # íƒ‘ìŠ¹í•­(Embarked) ì²˜ë¦¬
        ds = self.embarked_nominal(ds)
        
        # Age ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (Pclassì™€ Sex ê¸°ë°˜)
        ds = self.fill_age_by_group(ds)
        
        # Fare ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (Pclass ê¸°ë°˜)
        ds = self.fill_fare_by_pclass(ds)
        
        # ì¶”ê°€ íŒŒìƒë³€ìˆ˜ ìƒì„± (Age, Fare ì›ë³¸ ê°’ì´ í•„ìš”í•˜ë¯€ë¡œ drop ì „ì— ì¶”ê°€)
        ds = self.add_advanced_features(ds)
        
        # Age, Fare êµ¬ê°„í™”
        ds = self.age_ratio_enhanced(ds)
        ds = self.fare_ratio_enhanced(ds)
        
        # ì›ë³¸ Age, Fare ì»¬ëŸ¼ ì œê±°
        ds = self.drop_feature(ds, 'Age', 'Fare')
        
        # Pclass ì²˜ë¦¬
        ds = self.pclass_ordinal(ds)
        
        # ë¶ˆí•„ìš”í•œ ì›ë³¸ ì»¬ëŸ¼ ì œê±°
        ds = self.drop_feature(ds, 'SibSp', 'Parch', 'Cabin', 'Ticket')
        
        # íŠ¹ì„± ì¡°í•©(Feature Interaction) ì¶”ê°€
        ds = self.add_feature_interactions(ds)
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ ì›-í•« ì¸ì½”ë”©
        ds = self.apply_one_hot_encoding(ds)
        
        # ì •ê·œí™”
        ds = self.normalize_features(ds, scaler_type=scaler_type)
        
        # ìµœì¢… ë°ì´í„°ì…‹ ì»¬ëŸ¼ í™•ì¸
        print("\nğŸ” ìµœì¢… ì „ì²˜ë¦¬ ë°ì´í„°ì…‹ ì»¬ëŸ¼ ëª©ë¡:")
        print(ds.train.columns.tolist())

        return ds

    def drop_feature(self, dataset: TitanicSchema, *features: str) -> TitanicSchema:
        """íŠ¹ì • ì»¬ëŸ¼ì„ train/test ë°ì´í„°ì…‹ì—ì„œ ì œê±°í•©ë‹ˆë‹¤."""
        for col in features:
            if col in dataset.train.columns:
                dataset.train.drop(columns=col, inplace=True)
            if col in dataset.test.columns:
                dataset.test.drop(columns=col, inplace=True)
        return dataset
    
    def add_family_features(self, dataset: TitanicSchema) -> TitanicSchema:
        """SibSpì™€ Parchë¥¼ ì´ìš©í•´ FamilySizeì™€ IsAlone íŠ¹ì„±ì„ ì¶”ê°€í•©ë‹ˆë‹¤."""
        for df in [dataset.train, dataset.test]:
            # ê°€ì¡± í¬ê¸° = ë³¸ì¸ + í˜•ì œ/ë°°ìš°ì + ë¶€ëª¨/ìë…€
            df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
            # í˜¼ì ì—¬í–‰í•˜ëŠ”ì§€ ì—¬ë¶€
            df['IsAlone'] = (df['FamilySize'] == 1).astype(int)
        
        print("ğŸ‘ª ê°€ì¡± ê´€ë ¨ íŠ¹ì„± ì¶”ê°€ ì™„ë£Œ")
        return dataset

    def extract_title_from_name(self, dataset: TitanicSchema) -> TitanicSchema:
        """ì´ë¦„ì—ì„œ íƒ€ì´í‹€(ì§í•¨) ì¶”ì¶œ"""
        for df in [dataset.train, dataset.test]:
            df['Title'] = df['Name'].str.extract('([A-Za-z]+)\\.', expand=False)
        return dataset

    def remove_duplicate_title(self, dataset: TitanicSchema) -> dict:
        """íƒ€ì´í‹€ ì¤‘ë³µ ì œê±°í•˜ê³  ë§¤í•‘ ìƒì„±"""
        all_titles = set(dataset.train['Title'].unique()) | set(dataset.test['Title'].unique())
        print(f"ğŸ“Œ ì „ì²´ ì§í•¨ ëª©ë¡: {sorted(all_titles)}")
        return {
            'Mr': 1, 'Ms': 2, 'Mrs': 3, 'Master': 4,
            'Royal': 5, 'Rare': 6
        }

    def title_nominal(self, dataset: TitanicSchema, title_mapping: dict) -> TitanicSchema:
        """íƒ€ì´í‹€ì„ ê·¸ë£¹í™”í•˜ê³  ìˆ«ìë¡œ ë³€í™˜"""
        for df in [dataset.train, dataset.test]:
            df['Title'] = df['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')
            df['Title'] = df['Title'].replace(
                ['Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona', 'Mme'], 'Rare')
            df['Title'] = df['Title'].replace({'Mlle': 'Mr', 'Miss': 'Ms'})
            df['Title'] = df['Title'].map(title_mapping)
        return dataset

    def gender_nominal(self, dataset: TitanicSchema) -> TitanicSchema:
        """ì„±ë³„ì„ ìˆ«ìë¡œ ë³€í™˜"""
        mapping = {'male': 1, 'female': 2}
        for df in [dataset.train, dataset.test]:
            df['Gender'] = df['Sex'].map(mapping)
        return dataset

    def embarked_nominal(self, dataset: TitanicSchema) -> TitanicSchema:
        """íƒ‘ìŠ¹í•­ì„ ìˆ«ìë¡œ ë³€í™˜í•˜ê³  ê²°ì¸¡ì¹˜ëŠ” ìµœë¹ˆê°’ìœ¼ë¡œ ì±„ì›€"""
        mapping = {'S': 1, 'C': 2, 'Q': 3}
        for df in [dataset.train, dataset.test]:
            # ê²°ì¸¡ì¹˜ëŠ” ìµœë¹ˆê°’ 'S'ë¡œ ì±„ì›€
            df['Embarked'] = df['Embarked'].fillna('S')
            df['Embarked'] = df['Embarked'].map(mapping)
        return dataset
    
    def fill_age_by_group(self, dataset: TitanicSchema) -> TitanicSchema:
        """Pclassì™€ Sex ì¡°í•© ê·¸ë£¹ë³„ ì¤‘ì•™ê°’ìœ¼ë¡œ Age ê²°ì¸¡ì¹˜ë¥¼ ì±„ì›ë‹ˆë‹¤."""
        # trainê³¼ test ë°ì´í„°ë¥¼ ì„ì‹œë¡œ í•©ì³ì„œ ê·¸ë£¹ë³„ í†µê³„ê°’ ê³„ì‚°
        combined_df = pd.concat([dataset.train, dataset.test])[['Age', 'Pclass', 'Gender']]
        
        # ê·¸ë£¹ë³„ ì¤‘ì•™ê°’ ê³„ì‚°
        age_medians = combined_df.groupby(['Pclass', 'Gender'])['Age'].median()
        print("ğŸ“Š Pclass/Genderë³„ ë‚˜ì´ ì¤‘ì•™ê°’:")
        print(age_medians)
        
        # ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°
        for df in [dataset.train, dataset.test]:
            for (pclass, gender), age_median in age_medians.items():
                mask = (df['Age'].isna()) & (df['Pclass'] == pclass) & (df['Gender'] == gender)
                df.loc[mask, 'Age'] = age_median
                
            # ì—¬ì „íˆ ë‚¨ì•„ìˆëŠ” ê²°ì¸¡ì¹˜ëŠ” ì „ì²´ ì¤‘ì•™ê°’ìœ¼ë¡œ ì±„ì›€
            overall_median = combined_df['Age'].median()
            df['Age'] = df['Age'].fillna(overall_median)
        
        print("ğŸ‘´ ë‚˜ì´ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ")
        return dataset

    def age_ratio_enhanced(self, dataset: TitanicSchema) -> TitanicSchema:
        """ë‚˜ì´ë¥¼ ë” ì„¸ë¶„í™”ëœ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë§¤í•‘í•©ë‹ˆë‹¤ (ì´ 10ê°œ êµ¬ê°„)."""
        mapping = {
            'Infant': 1,           # 0-2ì„¸
            'Toddler': 2,          # 3-5ì„¸
            'Child': 3,            # 6-12ì„¸
            'Teenager': 4,         # 13-18ì„¸
            'YoungAdult': 5,       # 19-25ì„¸
            'Adult': 6,            # 26-35ì„¸
            'MidAgeAdult': 7,      # 36-45ì„¸
            'MiddleAged': 8,       # 46-55ì„¸
            'Senior': 9,           # 56-65ì„¸
            'Elderly': 10          # 66ì„¸ ì´ìƒ
        }
        
        # ë‚˜ì´ êµ¬ê°„ ê²½ê³„ ì •ì˜
        bins = [0, 2, 5, 12, 18, 25, 35, 45, 55, 65, 100]
        labels = list(mapping.keys())

        for df in [dataset.train, dataset.test]:
            df['AgeGroup'] = pd.cut(df['Age'], bins=bins, labels=labels, include_lowest=True)
            df['AgeGroup'] = df['AgeGroup'].map(mapping)
        
        print("ğŸ‘¶ ë‚˜ì´ ê·¸ë£¹ ì„¸ë¶„í™” ì™„ë£Œ (10ê°œ êµ¬ê°„)")
        return dataset
    
    def fill_fare_by_pclass(self, dataset: TitanicSchema) -> TitanicSchema:
        """Pclassë³„ í‰ê·  ìš”ê¸ˆìœ¼ë¡œ Fare ê²°ì¸¡ì¹˜ë¥¼ ì±„ì›ë‹ˆë‹¤."""
        # trainê³¼ test ë°ì´í„°ë¥¼ ì„ì‹œë¡œ í•©ì³ì„œ ê·¸ë£¹ë³„ í†µê³„ê°’ ê³„ì‚°
        combined_df = pd.concat([dataset.train, dataset.test])[['Fare', 'Pclass']]
        
        # Pclassë³„ í‰ê·  ìš”ê¸ˆ ê³„ì‚°
        fare_means = combined_df.groupby('Pclass')['Fare'].mean()
        print("ğŸ’² ê°ì‹¤ë“±ê¸‰ë³„ í‰ê·  ìš”ê¸ˆ:")
        print(fare_means)
        
        # ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°
        for df in [dataset.train, dataset.test]:
            for pclass, fare_mean in fare_means.items():
                mask = (df['Fare'].isna()) & (df['Pclass'] == pclass)
                df.loc[mask, 'Fare'] = fare_mean
                
            # ì—¬ì „íˆ ë‚¨ì•„ìˆëŠ” ê²°ì¸¡ì¹˜ëŠ” ì „ì²´ í‰ê· ìœ¼ë¡œ ì±„ì›€
            overall_mean = combined_df['Fare'].mean()
            df['Fare'] = df['Fare'].fillna(overall_mean)
        
        print("ğŸ’° ìš”ê¸ˆ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ")
        return dataset

    def fare_ratio_enhanced(self, dataset: TitanicSchema) -> TitanicSchema:
        """ìš”ê¸ˆì„ ë” ì„¸ë¶„í™”ëœ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ê³ , ìƒìœ„ 1% ëŸ­ì…”ë¦¬ êµ¬ê°„ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤ (ì´ 6ê°œ êµ¬ê°„)."""
        mapping = {
            'Free': 1,        # ë¬´ë£Œ
            'VeryLow': 2,     # ë§¤ìš° ì €ë ´
            'Low': 3,         # ì €ë ´
            'Mid': 4,         # ì¤‘ê°„
            'High': 5,        # ê³ ê°€
            'Luxury': 6       # ëŸ­ì…”ë¦¬ (ìƒìœ„ 1%)
        }
        
        # ì „ì²´ ë°ì´í„°ì˜ Fare í†µê³„ ê³„ì‚°
        combined_fare = pd.concat([dataset.train['Fare'], dataset.test['Fare']])
        
        # ìƒìœ„ 1% ê²½ê³„ê°’ ê³„ì‚°
        luxury_threshold = combined_fare.quantile(0.99)
        
        # 6ê°œ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ìœ„í•œ ê²½ê³„ê°’ ì„¤ì •
        bins = [0, 0.01, 7.5, 15, 30, luxury_threshold, combined_fare.max()]
        labels = list(mapping.keys())

        for df in [dataset.train, dataset.test]:
            df['FareGroup'] = pd.cut(df['Fare'], bins=bins, labels=labels, include_lowest=True)
            df['FareGroup'] = df['FareGroup'].map(mapping)
        
        print(f"ğŸ’³ ìš”ê¸ˆ ê·¸ë£¹ ì„¸ë¶„í™” ì™„ë£Œ (6ê°œ êµ¬ê°„, ëŸ­ì…”ë¦¬ ê¸°ì¤€: {luxury_threshold:.2f})")
        return dataset

    def pclass_ordinal(self, dataset: TitanicSchema) -> TitanicSchema:
        """ê°ì‹¤ë“±ê¸‰ ì²˜ë¦¬ (ì´ë¯¸ ìˆ«ìì´ë¯€ë¡œ ë³€ê²½ ì—†ìŒ)"""
        return dataset
    
    def add_feature_interactions(self, dataset: TitanicSchema) -> TitanicSchema:
        """íŠ¹ì„± ì¡°í•©ì„ í†µí•´ ìƒˆë¡œìš´ íŠ¹ì„±ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        for df in [dataset.train, dataset.test]:
            # Sexì™€ Pclass ì¡°í•©
            df['Gender_Pclass'] = df['Gender'].astype(str) + '_' + df['Pclass'].astype(str)
            gender_pclass_mapping = {val: idx for idx, val in enumerate(sorted(df['Gender_Pclass'].unique()), 1)}
            df['Gender_Pclass'] = df['Gender_Pclass'].map(gender_pclass_mapping)
            
            # AgeGroupê³¼ Pclass ì¡°í•©
            df['AgeGroup_Pclass'] = df['AgeGroup'].astype(str) + '_' + df['Pclass'].astype(str)
            age_pclass_mapping = {val: idx for idx, val in enumerate(sorted(df['AgeGroup_Pclass'].unique()), 1)}
            df['AgeGroup_Pclass'] = df['AgeGroup_Pclass'].map(age_pclass_mapping)
            
            # AgeGroupê³¼ FareGroup ì¡°í•© (ì¶”ê°€)
            df['AgeGroup_FareGroup'] = df['AgeGroup'].astype(str) + '_' + df['FareGroup'].astype(str)
            age_fare_mapping = {val: idx for idx, val in enumerate(sorted(df['AgeGroup_FareGroup'].unique()), 1)}
            df['AgeGroup_FareGroup'] = df['AgeGroup_FareGroup'].map(age_fare_mapping)
        
        print("ğŸ”„ íŠ¹ì„± ì¡°í•© ì¶”ê°€ ì™„ë£Œ")
        return dataset
    
    def apply_one_hot_encoding(self, dataset: TitanicSchema) -> TitanicSchema:
        """ë²”ì£¼í˜• ë³€ìˆ˜ì— ì›-í•« ì¸ì½”ë”©ì„ ì ìš©í•©ë‹ˆë‹¤."""
        # ì›-í•« ì¸ì½”ë”©í•  ë²”ì£¼í˜• íŠ¹ì„±
        categorical_features = ['Pclass', 'Title', 'Embarked']
        
        # scikit-learn ë²„ì „ í™•ì¸ ë° OneHotEncoder íŒŒë¼ë¯¸í„° ì„¤ì •
        try:
            current_version = version.parse(sklearn_version)
            version_boundary = version.parse("1.2")
            
            # ë²„ì „ì— ë”°ë¼ ì ì ˆí•œ ì¸ì½”ë” ì„ íƒ
            if current_version >= version_boundary:
                # 1.2 ì´ìƒ ë²„ì „: sparse_output ì‚¬ìš©
                print(f"ğŸ” scikit-learn ë²„ì „ {sklearn_version} ê°ì§€: sparse_output=False ì‚¬ìš©")
                encoder = OneHotEncoder(sparse_output=False, drop='first')
            else:
                # 1.1 ì´í•˜ ë²„ì „: sparse ì‚¬ìš©
                print(f"ğŸ” scikit-learn ë²„ì „ {sklearn_version} ê°ì§€: sparse=False ì‚¬ìš©")
                encoder = OneHotEncoder(sparse=False, drop='first')
        except Exception as e:
            # ë²„ì „ ë¹„êµ ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ fallback
            print(f"âš ï¸ ë²„ì „ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. sparse=Falseë¡œ fallback")
            encoder = OneHotEncoder(sparse=False, drop='first')
        
        # train ë°ì´í„°ë¡œ ì¸ì½”ë” í•™ìŠµ
        encoder.fit(dataset.train[categorical_features])
        
        # ë³€í™˜ ë° ìƒˆ ì»¬ëŸ¼ ì¶”ê°€
        for df_name, df in [('train', dataset.train), ('test', dataset.test)]:
            encoded_array = encoder.transform(df[categorical_features])
            
            # ìƒì„±ëœ ì›-í•« ì¸ì½”ë”© ì»¬ëŸ¼ì— ì´ë¦„ ë¶€ì—¬
            feature_names = []
            for i, feature in enumerate(categorical_features):
                categories = encoder.categories_[i][1:]  # ì²« ë²ˆì§¸ ì¹´í…Œê³ ë¦¬ëŠ” drop='first'ë¡œ ì œì™¸ë¨
                feature_names.extend([f"{feature}_{category}" for category in categories])
            
            # ì¸ì½”ë”© ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì›ë³¸ì— ì¶”ê°€
            encoded_df = pd.DataFrame(encoded_array, columns=feature_names, index=df.index)
            
            # ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì— ë³‘í•©
            result_df = pd.concat([df, encoded_df], axis=1)
            
            # ì›ë˜ ë²”ì£¼í˜• íŠ¹ì„±ì€ ì œê±°
            result_df.drop(columns=categorical_features, inplace=True)
            
            # ê²°ê³¼ ë‹¤ì‹œ í• ë‹¹
            if df_name == 'train':
                dataset.train = result_df
            else:
                dataset.test = result_df
        
        print("ğŸ”¢ ë²”ì£¼í˜• ë³€ìˆ˜ ì›-í•« ì¸ì½”ë”© ì™„ë£Œ")
        return dataset

    def normalize_features(self, dataset: TitanicSchema, scaler_type: str = 'standard') -> TitanicSchema:
        """
        ìˆ˜ì¹˜í˜• íŠ¹ì„±ì„ ì •ê·œí™”í•©ë‹ˆë‹¤.
        
        Args:
            dataset: ì •ê·œí™”í•  ë°ì´í„°ì…‹
            scaler_type: 'standard'(StandardScaler) ë˜ëŠ” 'minmax'(MinMaxScaler)
        
        Returns:
            ì •ê·œí™”ëœ ë°ì´í„°ì…‹
        """
        # ìŠ¤ì¼€ì¼ëŸ¬ ê°ì²´ ì„ íƒ
        if scaler_type.lower() == 'minmax':
            scaler = MinMaxScaler()
            print("ğŸ”„ MinMaxScalerë¡œ ë°ì´í„° ì •ê·œí™” ì§„í–‰")
        else:
            scaler = StandardScaler()
            print("ğŸ”„ StandardScalerë¡œ ë°ì´í„° ì •ê·œí™” ì§„í–‰")
        
        # ì •ê·œí™”í•  ìˆ˜ì¹˜í˜• íŠ¹ì„± ì„ íƒ (ì›-í•« ì¸ì½”ë”©ëœ ë³€ìˆ˜ëŠ” ì œì™¸)
        exclude_cols = ['Gender_Pclass', 'AgeGroup_Pclass', 'AgeGroup_FareGroup']  # ì´ë¯¸ ì¸ì½”ë”©ëœ ìƒí˜¸ì‘ìš© ë³€ìˆ˜
        numeric_features = dataset.train.select_dtypes(include=['int64', 'float64']).columns.tolist()
        numeric_features = [col for col in numeric_features if col not in exclude_cols]
        
        if not numeric_features:
            print("âš ï¸ ì •ê·œí™”í•  ìˆ˜ì¹˜í˜• íŠ¹ì„±ì´ ì—†ìŠµë‹ˆë‹¤.")
            return dataset
        
        try:
            # í•™ìŠµ ë°ì´í„°ë¡œ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ë° ë³€í™˜
            dataset.train[numeric_features] = scaler.fit_transform(dataset.train[numeric_features])
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ê°™ì€ ìŠ¤ì¼€ì¼ëŸ¬ ì ìš©
            dataset.test[numeric_features] = scaler.transform(dataset.test[numeric_features])
            
            print(f"âœ… ë°ì´í„° ì •ê·œí™” ì™„ë£Œ (ì´ {len(numeric_features)}ê°œ íŠ¹ì„±)")
        except Exception as e:
            print(f"âš ï¸ ë°ì´í„° ì •ê·œí™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ì •ê·œí™”ë¥¼ ê±´ë„ˆë›°ê³  ì›ë³¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        return dataset

    def add_advanced_features(self, dataset: TitanicSchema) -> TitanicSchema:
        """
        ê³ ê¸‰ íŒŒìƒë³€ìˆ˜ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤:
        1. Age Ã— Pclass â†’ Age_Pclass
        2. Fare Ã· FamilySize â†’ Fare_Per_Person
        3. AgeGroup Ã— FareGroup ì¡°í•© â†’ AgeGroup_FareGroup (add_feature_interactions ë©”ì„œë“œì—ì„œ ì¶”ê°€)
        """
        for df in [dataset.train, dataset.test]:
            # Age Ã— Pclass
            df['Age_Pclass'] = df['Age'] * df['Pclass']
            
            # Fare Ã· FamilySize (1ì¸ë‹¹ ì§€ë¶ˆìš”ê¸ˆ)
            # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ FamilySizeê°€ 0ì¸ ê²½ìš° 1ë¡œ ëŒ€ì²´
            df['Fare_Per_Person'] = df['Fare'] / df['FamilySize'].replace(0, 1)
        
        print("ğŸ” ê³ ê¸‰ íŒŒìƒë³€ìˆ˜ ì¶”ê°€ ì™„ë£Œ")
        return dataset
    
    def select_important_features(self, dataset: TitanicSchema) -> TitanicSchema:
        """
        RandomForest ê¸°ë°˜ìœ¼ë¡œ ì¤‘ìš” ë³€ìˆ˜ë§Œ ì„ íƒí•©ë‹ˆë‹¤.
        ì¤‘ìš”ë„ê°€ 0.01 ë¯¸ë§Œì¸ ë³€ìˆ˜ëŠ” ì œê±°í•©ë‹ˆë‹¤.
        """
        # í•™ìŠµ ë°ì´í„°ì— ì ìš©
        if 'train' not in self.__dict__ or self.dataset.train is None or self.dataset.label is None:
            print("âš ï¸ Feature Selectionì„ ìœ„í•œ í•™ìŠµ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return dataset
            
        try:
            print("\nğŸ“Š Feature Importance ê³„ì‚° ì¤‘...")
            
            # RandomForest ëª¨ë¸ í•™ìŠµ
            X_train = dataset.train
            y_train = self.dataset.label
            
            model = RandomForestClassifier(n_estimators=100, random_state=42)
            model.fit(X_train, y_train)
            
            # Feature Importance ê³„ì‚°
            importances = model.feature_importances_
            feature_names = X_train.columns
            
            # Feature Importance ë°ì´í„°í”„ë ˆì„ ìƒì„±
            feature_importance = pd.DataFrame({
                'Feature': feature_names,
                'Importance': importances
            }).sort_values(by='Importance', ascending=False)
            
            print("\nğŸ“Œ Feature Importance ìƒìœ„ 10ê°œ:")
            print(feature_importance.head(10))
            
            # ì¤‘ìš”ë„ê°€ 0.01 ë¯¸ë§Œì¸ ë³€ìˆ˜ ì„ íƒ
            low_importance_features = feature_importance[feature_importance['Importance'] < 0.01]['Feature'].tolist()
            
            if low_importance_features:
                print(f"\nğŸ—‘ï¸ ì œê±°í•  ë³€ìˆ˜ (ì¤‘ìš”ë„ < 0.01): {low_importance_features}")
                
                # í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ì¤‘ìš”ë„ê°€ ë‚®ì€ ë³€ìˆ˜ ì œê±°
                dataset.train = dataset.train.drop(columns=low_importance_features)
                dataset.test = dataset.test.drop(columns=low_importance_features)
                print(f"âœ… ì´ {len(low_importance_features)}ê°œ ë³€ìˆ˜ ì œê±° ì™„ë£Œ")
            else:
                print("\nâœ… ëª¨ë“  ë³€ìˆ˜ê°€ ì¶©ë¶„í•œ ì¤‘ìš”ë„ë¥¼ ê°€ì§‘ë‹ˆë‹¤ (>= 0.01).")
            
        except Exception as e:
            print(f"âš ï¸ Feature Selection ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ì›ë³¸ ë°ì´í„°ì…‹ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        return dataset

    # ë¨¸ì‹ ëŸ¬ë‹ : learning
    def create_k_fold(self, X, y, n_splits=10):
        """
        êµì°¨ê²€ì¦ì„ ìœ„í•œ StratifiedKFoldë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        10-Fold êµì°¨ê²€ì¦ì„ ì ìš©í•©ë‹ˆë‹¤.
        """
        return StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

    def create_random_variable(self):
        X_train = self.dataset.train
        y_train = self.dataset.label
        X_test = self.dataset.test
        return X_train, y_train, X_test
        
    def evaluate_model(self, model_class, X, y, **kwargs):
        """
        êµì°¨ê²€ì¦ì„ í†µí•´ ëª¨ë¸ì˜ í‰ê·  ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Args:
            model_class: ëª¨ë¸ í´ë˜ìŠ¤ ë˜ëŠ” ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
            X: íŠ¹ì„± ë°ì´í„°
            y: íƒ€ê²Ÿ ë ˆì´ë¸”
            **kwargs: ëª¨ë¸ ìƒì„± ì‹œ ì „ë‹¬í•  ì¶”ê°€ íŒŒë¼ë¯¸í„°
            
        Returns:
            float: êµì°¨ê²€ì¦ í‰ê·  ì •í™•ë„
        """
        kf = self.create_k_fold(X, y)
        accuracies = []
        
        for train_index, test_index in kf.split(X, y):
            X_fold_train, X_fold_test = X.iloc[train_index], X.iloc[test_index]
            y_fold_train, y_fold_test = y.iloc[train_index], y.iloc[test_index]
            
            # ëª¨ë¸ì´ ì´ë¯¸ ì¸ìŠ¤í„´ìŠ¤ì¸ì§€ í™•ì¸
            if isinstance(model_class, type):
                model = model_class(**kwargs)
            else:
                model = model_class
                
            model.fit(X_fold_train, y_fold_train)
            
            pred = model.predict(X_fold_test)
            accuracies.append(accuracy_score(y_fold_test, pred))
        
        return np.mean(accuracies)

    def accuracy_by_dtree(self, X, y):
        avg_accuracy = self.evaluate_model(DecisionTreeClassifier, X, y, random_state=42)
        print(f"ê²°ì •íŠ¸ë¦¬ í‰ê·  ì •í™•ë„: {avg_accuracy:.4f}")
        return avg_accuracy
    # if-elseì²˜ëŸ¼ ë°ì´í„°ë¥¼ ë¶„í• í•˜ë©° ì¡°ê±´ íŠ¸ë¦¬ë¥¼ ë§Œë“¦ (ì˜ˆ: ì„±ë³„ â†’ ë‚˜ì´ â†’ ê°ì‹¤ë“±ê¸‰).
    # ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” íŠ¹ì„±ë¶€í„° if-elseì²˜ëŸ¼ ë‚˜ëˆ„ë©° ìƒì¡´ ì—¬ë¶€ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. 
    # íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì—ì„œëŠ” ì„±ë³„, ë“±ê¸‰, ë‚˜ì´ ê°™ì€ ë³€ìˆ˜ë“¤ì´ ì£¼ìš” ë¶„ê¸° ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©
    # í›ˆë ¨ë°ì´í„°ì— ë§ì¶° í•™ìŠµì„ í•˜ë‹¤ë³´ë‹ˆ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ë©´ ì˜ˆì¸¡ ì •í™•ë„ê°€ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ (ex. ì—¬ì„±ì´ë©´ ìƒì¡´ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” ê²½ìš°)

    def accuracy_by_random_forest(self, X, y):
        avg_accuracy = self.evaluate_model(RandomForestClassifier, X, y, random_state=42)
        print(f"ëœë¤í¬ë ˆìŠ¤íŠ¸ í‰ê·  ì •í™•ë„: {avg_accuracy:.4f}")
        return avg_accuracy
    # ì—¬ëŸ¬ ê²°ì •íŠ¸ë¦¬ë¥¼ í•™ìŠµí•œ í›„ ê²°ê³¼ë¥¼ íˆ¬í‘œë¡œ ì¢…í•© (ì•™ìƒë¸”)

    def accuracy_by_naive_bayes(self, X, y):
        avg_accuracy = self.evaluate_model(GaussianNB, X, y)
        print(f"ë‚˜ì´ë¸Œë² ì´ì¦ˆ í‰ê·  ì •í™•ë„: {avg_accuracy:.4f}")
        return avg_accuracy
    # ê° íŠ¹ì„±ì´ ë…ë¦½ì´ë¼ê³  ê°€ì •í•˜ê³  í™•ë¥  ê¸°ë°˜ìœ¼ë¡œ ë¶„ë¥˜

    def accuracy_by_knn(self, X, y):
        avg_accuracy = self.evaluate_model(KNeighborsClassifier, X, y, n_neighbors=5)
        print(f"K-ìµœê·¼ì ‘ì´ì›ƒ í‰ê·  ì •í™•ë„: {avg_accuracy:.4f}")
        return avg_accuracy
    # ìƒˆ ë°ì´í„°ê°€ ì£¼ë³€ ì´ì›ƒ(Kê°œ)ê³¼ ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ì§€ë¡œ íŒë‹¨

    def accuracy_by_svm(self, X, y):
        avg_accuracy = self.evaluate_model(SVC, X, y, random_state=42)
        print(f"ì„œí¬íŠ¸ë²¡í„°ë¨¸ì‹  í‰ê·  ì •í™•ë„: {avg_accuracy:.4f}")
        return avg_accuracy
    # ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ëŠ” ê°€ì¥ ë„“ì€ ê²½ê³„(ì´ˆí‰ë©´)ë¥¼ ì°¾ì•„ ë¶„ë¥˜

    def accuracy_by_voting_ensemble(self, X, y):
        """
        ëœë¤í¬ë ˆìŠ¤íŠ¸, KNN, SVMì„ ì¡°í•©í•œ ì•™ìƒë¸” ëª¨ë¸(VotingClassifier)ì˜ ì •í™•ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.
        """
        # ê°œë³„ ëª¨ë¸ ì •ì˜
        rf = RandomForestClassifier(random_state=42)
        knn = KNeighborsClassifier(n_neighbors=5)
        svm = SVC(probability=True, random_state=42)  # soft votingì„ ìœ„í•´ probability=True ì„¤ì •
        
        # VotingClassifier ìƒì„± (soft voting)
        voting_clf = VotingClassifier(
            estimators=[('rf', rf), ('knn', knn), ('svm', svm)],
            voting='soft'
        )
        
        # êµì°¨ê²€ì¦ ì •í™•ë„ ê³„ì‚°
        avg_accuracy = self.evaluate_model(voting_clf, X, y)
        print(f"ì•™ìƒë¸”(Voting) í‰ê·  ì •í™•ë„: {avg_accuracy:.4f}")
        return avg_accuracy
    
    def optimize_random_forest(self, X, y):
        """
        GridSearchCVë¥¼ ì‚¬ìš©í•˜ì—¬ ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤.
        """
        print("\nğŸ” ëœë¤í¬ë ˆìŠ¤íŠ¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘...")
        
        # íƒìƒ‰í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜
        param_grid = {
            'n_estimators': [100, 200, 300],
            'max_depth': [5, 10, 15, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }
        
        # ê¸°ë³¸ ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ ìƒì„±
        rf = RandomForestClassifier(random_state=42)
        
        # GridSearchCV ì„¤ì •
        grid_search = GridSearchCV(
            estimator=rf,
            param_grid=param_grid,
            cv=self.create_k_fold(X, y),
            scoring='accuracy',
            n_jobs=-1,  # ëª¨ë“  CPU ì‚¬ìš©
            verbose=1
        )
        
        try:
            # ê·¸ë¦¬ë“œ ì„œì¹˜ ìˆ˜í–‰
            grid_search.fit(X, y)
            
            # ìµœì  íŒŒë¼ë¯¸í„° ë° ì ìˆ˜ ì¶œë ¥
            print(f"\nâœ… ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
            print(f"ğŸ† ìµœì  êµì°¨ê²€ì¦ ì •í™•ë„: {grid_search.best_score_:.4f}")
            
            # ìµœì  ëª¨ë¸ ì„¤ì •
            best_rf = grid_search.best_estimator_
            
            # êµì°¨ê²€ì¦ ì •í™•ë„ ê³„ì‚°
            kf = self.create_k_fold(X, y)
            accuracies = []
            
            for train_index, test_index in kf.split(X, y):
                X_fold_train, X_fold_test = X.iloc[train_index], X.iloc[test_index]
                y_fold_train, y_fold_test = y.iloc[train_index], y.iloc[test_index]
                
                # ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ì´ë¯€ë¡œ ì¬í•™ìŠµí•˜ì§€ ì•ŠìŒ
                pred = best_rf.predict(X_fold_test)
                accuracies.append(accuracy_score(y_fold_test, pred))
            
            avg_accuracy = np.mean(accuracies)
            print(f"ìµœì í™”ëœ ëœë¤í¬ë ˆìŠ¤íŠ¸ í‰ê·  ì •í™•ë„: {avg_accuracy:.4f}")
            
            return avg_accuracy, best_rf
            
        except Exception as e:
            print(f"âš ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ê¸°ë³¸ ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            return self.accuracy_by_random_forest(X, y), None

    def find_best_model(self):
        print("ğŸ” ìµœì  ëª¨ë¸ ì„ íƒ ì‹œì‘...")
        X, y, _ = self.create_random_variable()
        
        # ê° ëª¨ë¸ í‰ê°€
        models = {
            "ê²°ì •íŠ¸ë¦¬": self.accuracy_by_dtree,
            "ëœë¤í¬ë ˆìŠ¤íŠ¸": self.accuracy_by_random_forest,
            "ë‚˜ì´ë¸Œë² ì´ì¦ˆ": self.accuracy_by_naive_bayes,
            "K-ìµœê·¼ì ‘ì´ì›ƒ": self.accuracy_by_knn,
            "ì„œí¬íŠ¸ë²¡í„°ë¨¸ì‹ ": self.accuracy_by_svm,
            "XGBoost": self.accuracy_by_xgboost,
            "LightGBM": self.accuracy_by_lightgbm,
            "ì•™ìƒë¸”(Voting)": self.accuracy_by_voting_ensemble
        }
        
        results = {}
        for name, func in models.items():
            print(f"\n{name} í‰ê°€ ì¤‘...")
            accuracy = func(X, y)
            results[name] = accuracy
        
        # ëœë¤í¬ë ˆìŠ¤íŠ¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
        print("\nëœë¤í¬ë ˆìŠ¤íŠ¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘...")
        rf_accuracy, best_rf = self.optimize_random_forest(X, y)
        results["ìµœì í™”ëœ ëœë¤í¬ë ˆìŠ¤íŠ¸"] = rf_accuracy
        
        # ì •í™•ë„ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        sorted_results = dict(sorted(results.items(), key=lambda x: x[1], reverse=True))
        
        # ê²°ê³¼ í‘œ í˜•íƒœë¡œ ì¶œë ¥
        print("\nğŸ“Š ëª¨ë¸ ì •í™•ë„ ìˆœìœ„:")
        print("=" * 40)
        print(f"{'ëª¨ë¸ëª…':<25} {'ì •í™•ë„':>10}")
        print("-" * 40)
        for i, (name, accuracy) in enumerate(sorted_results.items(), 1):
            print(f"{i:2d}. {name:<22} {accuracy:.4f}")
        print("=" * 40)
        
        best_model = list(sorted_results.keys())[0]
        best_accuracy = sorted_results[best_model]
        
        print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model} (ì •í™•ë„: {best_accuracy:.4f})")
        return best_model, best_accuracy
    
    def accuracy_by_xgboost(self, X, y):
        """XGBoost ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤."""
        if XGBClassifier is None:
            print("âš ï¸ XGBoost ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return 0.0
        
        try:
            # ê¸°ë³¸ XGBoost ëª¨ë¸ ìƒì„±
            avg_accuracy = self.evaluate_model(
                XGBClassifier, X, y, 
                random_state=42,
                use_label_encoder=False,  # ê²½ê³  ë°©ì§€
                eval_metric='logloss'     # ê²½ê³  ë°©ì§€
            )
            print(f"XGBoost í‰ê·  ì •í™•ë„: {avg_accuracy:.4f}")
            return avg_accuracy
        except Exception as e:
            print(f"âš ï¸ XGBoost í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return 0.0
    
    def accuracy_by_lightgbm(self, X, y):
        """LightGBM ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤."""
        if LGBMClassifier is None:
            print("âš ï¸ LightGBM ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return 0.0
        
        try:
            # ê¸°ë³¸ LightGBM ëª¨ë¸ ìƒì„±
            avg_accuracy = self.evaluate_model(
                LGBMClassifier, X, y, 
                random_state=42,
                verbose=-1  # ì¶œë ¥ ìµœì†Œí™”
            )
            print(f"LightGBM í‰ê·  ì •í™•ë„: {avg_accuracy:.4f}")
            return avg_accuracy
        except Exception as e:
            print(f"âš ï¸ LightGBM í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return 0.0
    
    def create_voting_ensemble(self):
        """
        ëœë¤í¬ë ˆìŠ¤íŠ¸, KNN, SVM ëª¨ë¸ì„ ì¡°í•©í•œ VotingClassifier(ì•™ìƒë¸” ë¶„ë¥˜ê¸°)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        rf = RandomForestClassifier(random_state=42)
        knn = KNeighborsClassifier(n_neighbors=5)
        svm = SVC(probability=True, random_state=42)

        voting_clf = VotingClassifier(
            estimators=[('rf', rf), ('knn', knn), ('svm', svm)],
            voting='soft'
        )
        return voting_clf
    
    
    
    
    