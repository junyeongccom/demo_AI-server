from app.domain.service.titanic_service import TitanicService
from app.domain.model.titanic_schema import TitanicSchema
import pandas as pd
import os
from pathlib import Path

'''
print(f'ê²°ì •íŠ¸ë¦¬ í™œìš©í•œ ê²€ì¦ ì •í™•ë„ {None}')
print(f'ëœë¤í¬ë ˆìŠ¤íŠ¸ í™œìš©í•œ ê²€ì¦ ì •í™•ë„ {None}')
print(f'ë‚˜ì´ë¸Œë² ì´ì¦ˆ í™œìš©í•œ ê²€ì¦ ì •í™•ë„ {None}')
print(f'KNN í™œìš©í•œ ê²€ì¦ ì •í™•ë„ {None}')
print(f'SVM í™œìš©í•œ ê²€ì¦ ì •í™•ë„ {None}')
'''
class TitanicController:
    def __init__(self):
        self.titanic_service = TitanicService()

    def preprocess(self) -> TitanicSchema:
        print("ğŸ“¦ ì „ì²˜ë¦¬ ì‹œì‘ (Service ë‚´ë¶€ì—ì„œ ì „ì²´ ìˆ˜í–‰)")
        # ê²½ë¡œ, íŒŒì¼ëª…ì€ ë‚´ë¶€ì—ì„œ ì„¤ì •í•˜ê±°ë‚˜ ì¸ìë¡œ ì „ë‹¬
        ds = self.titanic_service.preprocess('train.csv', 'test.csv')
        print("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ")
        return ds
    
    def learning(self):
        """
        ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ì§„í–‰ì„ ìœ„í•œ ê°„ë‹¨í•œ ë˜í¼ ë©”ì„œë“œ
        evaluation()ë§Œ í˜¸ì¶œí•˜ì—¬ ê²°ê³¼ ë°˜í™˜
        """
        print("ğŸ§  ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ í˜¸ì¶œ")
        return self.evaluation()

    def evaluation(self):
        """
        ì—¬ëŸ¬ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  êµì°¨ê²€ì¦í•˜ì—¬ ì •í™•ë„ë¥¼ ë¹„êµ í‰ê°€
        """
        print("ğŸ” ëª¨ë¸ í‰ê°€ ì‹œì‘...")
        
        # ì „ì²˜ë¦¬ëœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        X, y, X_test = self.titanic_service.create_random_variable()
        
        # Xê°€ Noneì¸ì§€ í™•ì¸
        if X is None or y is None:
            print("âš ï¸ ë°ì´í„° ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            return None, None, 0.0
        
        # Feature Selection ì ìš©
        print("\nğŸ” Feature Selectionì„ í†µí•œ ë³€ìˆ˜ ìµœì í™” ì‹œì‘...")
        print("ì¤‘ìš”ë„ê°€ ë‚®ì€ í”¼ì²˜(< 0.01)ë¥¼ ì œê±°í•©ë‹ˆë‹¤.")
        X, X_test, removed_features = self.apply_feature_selection(X, y, X_test)
        
        # Feature Selection í›„ Xê°€ Noneì¸ì§€ ë‹¤ì‹œ í™•ì¸
        if X is None:
            print("âš ï¸ Feature Selection ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            return None, None, 0.0
        
        if removed_features:
            print(f"\nğŸ—‘ï¸ ì œê±°ëœ í”¼ì²˜ ëª©ë¡: {removed_features}")
        else:
            print("\nâœ… ëª¨ë“  í”¼ì²˜ê°€ ì¤‘ìš”ë„ ê¸°ì¤€ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤.")
        
        # Feature Selection í›„ ë‚¨ì€ í”¼ì²˜ í™•ì¸
        print(f"\nğŸ” Feature Selection í›„ ë‚¨ì€ í”¼ì²˜ ëª©ë¡ ({len(X.columns)}ê°œ):")
        print(X.columns.tolist())
        
        # ê° ì•Œê³ ë¦¬ì¦˜ë³„ ì •í™•ë„ ê³„ì‚°
        model_results = {}
        print("\nê° ëª¨ë¸ í‰ê°€ ì¤‘...")
        
        model_results["ê²°ì •íŠ¸ë¦¬"] = self.titanic_service.accuracy_by_dtree(X, y)
        model_results["ëœë¤í¬ë ˆìŠ¤íŠ¸"] = self.titanic_service.accuracy_by_random_forest(X, y) 
        model_results["ë‚˜ì´ë¸Œë² ì´ì¦ˆ"] = self.titanic_service.accuracy_by_naive_bayes(X, y)
        model_results["K-ìµœê·¼ì ‘ì´ì›ƒ"] = self.titanic_service.accuracy_by_knn(X, y)
        model_results["ì„œí¬íŠ¸ë²¡í„°ë¨¸ì‹ "] = self.titanic_service.accuracy_by_svm(X, y)
        model_results["XGBoost"] = self.titanic_service.accuracy_by_xgboost(X, y)
        model_results["LightGBM"] = self.titanic_service.accuracy_by_lightgbm(X, y)
        model_results["ì•™ìƒë¸”(Voting)"] = self.titanic_service.accuracy_by_voting_ensemble(X, y)
        
        # ëœë¤í¬ë ˆìŠ¤íŠ¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
        print("\nëœë¤í¬ë ˆìŠ¤íŠ¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘...")
        rf_accuracy, best_rf = self.titanic_service.optimize_random_forest(X, y)
        model_results["ìµœì í™”ëœ ëœë¤í¬ë ˆìŠ¤íŠ¸"] = rf_accuracy
        
        # ì •í™•ë„ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        sorted_results = dict(sorted(model_results.items(), key=lambda x: x[1], reverse=True))
        
        # ê²°ê³¼ í‘œ í˜•íƒœë¡œ ì¶œë ¥
        print("\nğŸ“Š ëª¨ë¸ ì •í™•ë„ ìˆœìœ„:")
        print("=" * 40)
        print(f"{'ëª¨ë¸ëª…':<25} {'ì •í™•ë„':>10}")
        print("-" * 40)
        for i, (name, accuracy) in enumerate(sorted_results.items(), 1):
            print(f"{i:2d}. {name:<22} {accuracy:.4f}")
        print("=" * 40)
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
        best_model = list(sorted_results.keys())[0]
        best_accuracy = sorted_results[best_model]
        
        print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model} (ì •í™•ë„: {best_accuracy:.4f})")
        
        # ì´ ë‘ ê°’ì„ selfì— ì €ì¥í•˜ì—¬ ë‹¤ë¥¸ ë©”ì„œë“œì—ì„œë„ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•¨
        self.selected_features = X.columns.tolist()
        self.removed_features = removed_features
        
        return sorted_results, best_model, best_accuracy
    
    def apply_feature_selection(self, X, y, X_test):
        """
        Feature Importanceë¥¼ ê³„ì‚°í•˜ê³  ì¤‘ìš”ë„ê°€ ë‚®ì€ í”¼ì²˜ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
        
        Args:
            X: í•™ìŠµ ë°ì´í„°
            y: íƒ€ê²Ÿ ë ˆì´ë¸”
            X_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°
            
        Returns:
            tuple: (ë³€í™˜ëœ X, ë³€í™˜ëœ X_test, ì œê±°ëœ í”¼ì²˜ ëª©ë¡)
        """
        from sklearn.ensemble import RandomForestClassifier
        import pandas as pd
        
        try:
            # RandomForest ëª¨ë¸ í•™ìŠµ
            model = RandomForestClassifier(n_estimators=100, random_state=42)
            model.fit(X, y)
            
            # Feature Importance ê³„ì‚°
            importances = model.feature_importances_
            feature_names = X.columns
            
            # Feature Importance ë°ì´í„°í”„ë ˆì„ ìƒì„±
            feature_importance = pd.DataFrame({
                'Feature': feature_names,
                'Importance': importances
            }).sort_values(by='Importance', ascending=False)
            
            print("\nğŸ“Œ Feature Importance ìƒìœ„ 10ê°œ:")
            print(feature_importance.head(10))
            
            # ì¤‘ìš”ë„ê°€ 0.01 ë¯¸ë§Œì¸ ë³€ìˆ˜ ì„ íƒ
            low_importance_features = feature_importance[feature_importance['Importance'] < 0.01]['Feature'].tolist()
            
            if low_importance_features:
                # ì¤‘ìš”ë„ê°€ ë‚®ì€ ë³€ìˆ˜ ì œê±°
                X_filtered = X.drop(columns=low_importance_features)
                X_test_filtered = X_test.drop(columns=low_importance_features)
                print(f"âœ… ì´ {len(low_importance_features)}ê°œ ë³€ìˆ˜ ì œê±° ì™„ë£Œ")
                return X_filtered, X_test_filtered, low_importance_features
            else:
                print("âœ… ëª¨ë“  ë³€ìˆ˜ê°€ ì¶©ë¶„í•œ ì¤‘ìš”ë„ë¥¼ ê°€ì§‘ë‹ˆë‹¤ (>= 0.01)")
                return X, X_test, []
        
        except Exception as e:
            print(f"âš ï¸ Feature Selection ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ì›ë³¸ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return X, X_test, []

    def submit(self):
        """
        í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³  submission íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.
        Feature Selectionì„ í†µí•´ ì„ íƒëœ íŠ¹ì„±ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        try:
            # ì „ì²˜ë¦¬ëœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            X, y, X_test = self.titanic_service.create_random_variable()
            
            # Feature Selection ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
            if hasattr(self, 'selected_features') and self.selected_features:
                print(f"\nğŸ” Feature Selection ê²°ê³¼ë¥¼ ì ìš©í•˜ì—¬ {len(self.selected_features)}ê°œì˜ í”¼ì²˜ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                X = X[self.selected_features]
                X_test = X_test[self.selected_features]
                
                if hasattr(self, 'removed_features') and self.removed_features:
                    print(f"ğŸ—‘ï¸ ì œì™¸ëœ í”¼ì²˜: {self.removed_features}")
            else:
                print("\nâš ï¸ Feature Selection ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  í”¼ì²˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                
            print("\nğŸ“Š ì˜ˆì¸¡ ëª¨ë¸(ì•™ìƒë¸” ë³´íŒ…) í•™ìŠµ ì¤‘...")
            
            # ì•™ìƒë¸” ë³´íŒ… ë¶„ë¥˜ê¸°ë¡œ ë³€ê²½ (ìµœê³  ì„±ëŠ¥ ë³´ì¥)
            model = self.titanic_service.create_voting_ensemble()
            model.fit(X, y)
            
            print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰
            predictions = model.predict(X_test)
            
            # submission íŒŒì¼ ìƒì„±
            submission = pd.DataFrame({
                'PassengerId': self.titanic_service.test_ids,
                'Survived': predictions.astype(int)
            })
            
            # submission íŒŒì¼ ì €ì¥
            submission_path = 'app/updated_data/submission.csv'
            submission.to_csv(submission_path, index=False)
            
            print(f"\nğŸ“„ ì˜ˆì¸¡ ê²°ê³¼ê°€ {submission_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print(f"ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½: ìƒì¡´={sum(predictions)}, ì‚¬ë§={len(predictions)-sum(predictions)}")
            
            return submission
        
        except Exception as e:
            print(f"âš ï¸ ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

